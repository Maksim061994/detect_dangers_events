version: "3.7"

services:

  data_science_ui:
    image: data_science_ui:latest
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data_science_ui
    env_file: .env
    depends_on:
      - redis
      - clickhouse
      - postgresql
    restart: unless-stopped
    hostname: superset
    environment:
      - TZ=Europe/Moscow
    logging:
      driver: "json-file"
      options:
        max-size: 10m
        max-file: "5"
    ports:
      - 8089:8088
    volumes:
      - ./configs/config.py:/app/superset/config.py
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8088"]
      interval: 10s
      timeout: 10s
      retries: 5
    networks:
      data_science:
        ipv4_address: 193.168.15.2

  celery:
    image: data_science_ui:latest
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data_science_ui_worker
    environment:
      - TZ=Europe/Moscow
    logging:
      driver: "json-file"
      options:
        max-size: 10m
        max-file: "5"
    volumes:
      - ./configs/config.py:/app/superset/config.py
    command: celery -A superset.tasks.celery_app:app worker -E --pool=prefork -O fair -c 4
    env_file: .env
    restart: unless-stopped
    networks:
      data_science:
        ipv4_address: 193.168.15.9

  beat:
    image: data_science_ui:latest
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data_science_ui_beat
    environment:
      - TZ=Europe/Moscow
    logging:
      driver: "json-file"
      options:
        max-size: 10m
        max-file: "5"
    volumes:
      - ./configs/config.py:/app/superset/config.py
    command: celery -A superset.tasks.celery_app:app beat --pidfile= -f /tmp/celery_beat.log -s /tmp/celerybeat-schedule
    env_file: .env
    restart: unless-stopped
    networks:
      data_science:
        ipv4_address: 193.168.15.10

  flower:
    image: data_science_ui:latest
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data_science_ui_flower
    environment:
      - TZ=Europe/Moscow
    logging:
      driver: "json-file"
      options:
        max-size: 10m
        max-file: "5"
    ports:
      - 5555:5555
    volumes:
      - ./configs/config.py:/app/superset/config.py
    command: celery -A superset.tasks.celery_app:app flower
    env_file: .env
    restart: unless-stopped
    networks:
      data_science:
        ipv4_address: 193.168.15.11

  redis:
    image: redis:7
    container_name: data_science_cache
    restart: unless-stopped
    hostname: redis
    environment:
      - TZ=Europe/Moscow
    ports:
      - 6379:6379
    volumes:
      - ./redis:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    networks:
      data_science:
        ipv4_address: 193.168.15.3
      
  clickhouse:
    image: clickhouse/clickhouse-server:23.4.2.11
    container_name: data_science_clickhouse
    env_file: .env
    restart: unless-stopped
    hostname: clickhouse
    environment:
      - TZ=Europe/Moscow
      - ALLOW_EMPTY_PASSWORD=no
      - CLICKHOUSE_DB=data_science
      - CLICKHOUSE_USER=hakaton
      - CLICKHOUSE_PASSWORD=hakatoN420M3haK
    ports:
      - "8123:8123"
      - "9010:9000"
    volumes:
      - ./ch:/var/lib/clickhouse
      - ./logs/clickhouse:/var/log/clickhouse-server
      - ./configs/clickhouse:/etc/clickhouse-server
    cap_add: 
      - SYS_NICE
      - NET_ADMIN
      - IPC_LOCK
    ulimits:
      nproc: 65535
      nofile:
        soft: 65535
        hard: 262144
    networks:
      data_science:
        ipv4_address: 193.168.15.4

  postgresql:
    restart: always
    image: postgres:15.3
    container_name: data_science_postgresql
    hostname: postgresql
    env_file: .env
    ports:
      - "${DB_PORT}:${DB_PORT}"
    environment:
      - POSTGRES_DB=${DB_NAME}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PW}
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - /dev/urandom:/dev/random
      - ./pgdata:/var/lib/postgresql/data
      - ./db/init.sql:/docker-entrypoint-initdb.d/10-init.sql
    command: >
     postgres
       -c port=5433
       -c max_connections=500
       -c shared_buffers=4GB
       -c work_mem=16MB
       -c maintenance_work_mem=512MB
       -c random_page_cost=1.1
       -c effective_cache_size=4GB
       -c log_destination=stderr
       -c logging_collector=on
       -c log_filename='postgresql-%G-%m.log'
       -c log_truncate_on_rotation=off
       -c log_rotation_age=10d
       -c client_min_messages=warning
       -c log_min_messages=warning
       -c log_min_error_statement=error
       -c log_line_prefix='%t %u@%r:%d [%p] '
       -c log_min_duration_statement=200ms
       -c log_timezone='Europe/Moscow'
       -c temp_file_limit=10GB
       -c idle_in_transaction_session_timeout=30s
       -c lock_timeout=0
       -c statement_timeout=6000s
       -c shared_preload_libraries=pg_stat_statements
       -c pg_stat_statements.max=10000
       -c pg_stat_statements.track=all
       -c timezone='Europe/Moscow'
       -c track_counts=on
       -c autovacuum=on
       -c track_activities=on
       -c track_io_timing=on       
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres", "-p", "5433" ]
      interval: 5s
      retries: 5
    networks:
      data_science:
        ipv4_address: 193.168.15.5

  api_ai:
    image: ai_api:latest
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data_science_api_ai
    command: gunicorn app.main:app -k uvicorn.workers.UvicornWorker --log-config=logconf.ini
    environment:
      - ENVIRONMENT=dev
      - TZ=Europe/Moscow
      - DEVICE=cuda:1
      - TOKENIZERS_PARALLELISM=true
    ports:
      - 3025:8000
    restart: always
    volumes:
      - ./app:/opt/app
      - ./gunicorn.conf.py:/opt/gunicorn.conf.py
      - ./.env.dev:/opt/.env.dev
    logging:
      driver: "json-file"
      options:
          max-size: "10m"
          max-file: "10"
    deploy:
      resources:
        limits:
          cpus: '6'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      data_science:
        ipv4_address: 193.168.15.6

networks:
  data_science:
    driver: bridge
    ipam:
      config:
        - subnet: 193.168.15.0/24